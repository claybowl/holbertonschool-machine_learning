{"cells":[{"cell_type":"markdown","metadata":{"id":"OSChsR4Oz9Sz"},"source":["# Task 6-baysian_optimization by Clayton Christian"]},{"cell_type":"markdown","metadata":{"id":"8k4MHtrKxYhy"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11831,"status":"ok","timestamp":1688094064874,"user":{"displayName":"Clayton Christian","userId":"10166640897750611848"},"user_tz":300},"id":"GeyzItfVrPfN","outputId":"12c23cfb-5c5d-4757-8718-a2f9fb523616"},"outputs":[],"source":["import tensorflow as tf\n","# !pip install GPyOpt\n","from GPyOpt.methods import BayesianOptimization\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import csv"]},{"cell_type":"markdown","metadata":{"id":"HgzR6FEmxg6u"},"source":["# Load the datasets:\n","We're using a complete collection of William Shakespeare's works, The Book of Mormon, The Gospel of Buddha, Marcus Arelias Meditations, The Old Testament, and the Koran. All files are uploaded as .txt files."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1688094852028,"user":{"displayName":"Clayton Christian","userId":"10166640897750611848"},"user_tz":300},"id":"NTjUu-LAruYM"},"outputs":[],"source":["text_files = ['data/book_of_mormon.txt', 'data/gospel_of_buddha.txt', 'data/meditations.txt',\n","              'data/old_testament.txt', 'data/t8.shakespeare.txt', 'data/the_koran.txt']\n","\n","for text_file in text_files:\n","\n","    # Load and preprocess the text data\n","    with open(text_file, 'r') as file:\n","        text = file.read()\n"]},{"cell_type":"markdown","metadata":{"id":"wGuJFuevx27H"},"source":["# Preprocess the Data\n","\n","Basically, converting the characters to integers to create an input-output sequence for training our RNN."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12757,"status":"ok","timestamp":1688094870306,"user":{"displayName":"Clayton Christian","userId":"10166640897750611848"},"user_tz":300},"id":"moYfUq8os57l"},"outputs":[],"source":["chars = sorted(list(set(text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","\n","# Create input-output sequences\n","seq_length = 100\n","dataX, dataY = [], []\n","for i in range(0, len(text) - seq_length, 1):\n","    seq_in = text[i:i + seq_length]\n","    seq_out = text[i + seq_length]\n","    dataX.append([char_to_int[char] for char in seq_in])\n","    dataY.append(char_to_int[seq_out])"]},{"cell_type":"markdown","metadata":{"id":"CHUn2e2nyBjl"},"source":["## We're using a simple RNN model in TensorFlow, with the parameters defined by the school project."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":827,"status":"ok","timestamp":1688094994858,"user":{"displayName":"Clayton Christian","userId":"10166640897750611848"},"user_tz":300},"id":"mZ82AVqDs7gb"},"outputs":[],"source":["def create_model(learning_rate, num_units, dropout_rate):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(\n","            len(chars), num_units, input_length=seq_length),\n","        tf.keras.layers.SimpleRNN(num_units, return_sequences=True),\n","        tf.keras.layers.Dropout(dropout_rate),\n","        tf.keras.layers.SimpleRNN(num_units),\n","        tf.keras.layers.Dropout(dropout_rate),\n","        tf.keras.layers.Dense(len(chars), activation='softmax')\n","    ])\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(loss='sparse_categorical_crossentropy',\n","                  optimizer=optimizer, metrics=['accuracy'])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"-v_FU1HByLul"},"source":["## Here we're defining the **obective function** for our **baysian optimization**. We're defining a function that **takes the hyperparameters as input and returns our validation loss**."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1688094997896,"user":{"displayName":"Clayton Christian","userId":"10166640897750611848"},"user_tz":300},"id":"Q3VE10Zls9aL"},"outputs":[],"source":["def objective_function(hyperparameters):\n","    learning_rate, num_units, dropout_rate, batch_size = hyperparameters[0]\n","    num_units = int(num_units)\n","    batch_size = int(batch_size)\n","\n","    model = create_model(learning_rate, num_units, dropout_rate)\n","\n","    # Use early stopping\n","    early_stopping = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss', patience=3)\n","\n","    # Train the model\n","    history = model.fit(dataX, dataY, validation_split=0.2, epochs=30,\n","                        batch_size=batch_size, verbose=0, callbacks=[early_stopping])\n","\n","    # Return the validation loss of the last epoch\n","    return history.history['val_loss'][-1]"]},{"cell_type":"markdown","metadata":{"id":"g-mDk0ZhygPV"},"source":["# Here we are running our **Baysian Optimization**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5y1uSeDHtAVx"},"outputs":[],"source":["domain = [\n","    {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-4, 1e-2)},\n","    {'name': 'num_units', 'type': 'discrete', 'domain': (32, 64, 128, 256)},\n","    {'name': 'dropout_rate', 'type': 'continuous', 'domain': (0.1, 0.5)},\n","    {'name': 'batch_size', 'type': 'discrete', 'domain': (32, 64, 128)}\n","]\n","\n","optimizer = BayesianOptimization(\n","    f=objective_function, domain=domain, max_iter=30)\n","optimizer.run_optimization()"]},{"cell_type":"markdown","metadata":{"id":"7UEQJa1_yjce"},"source":["## Plot the Convergence of the optimization process to observe **validation loss decreases** over time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzHIexTStCM2"},"outputs":[],"source":["optimizer.plot_convergence()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4w-5u5AXyuVo"},"source":["# Save optimization report as .txt file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kO8YhMZZtDQm"},"outputs":[],"source":["with open('bayes_opt.txt', 'w') as file:\n","    file.write(str(optimizer.X))\n","    file.write('\\n')\n","    file.write(str(optimizer.Y))"]},{"cell_type":"markdown","metadata":{"id":"bHqHUZWUyySE"},"source":["# Save optimizztion report as .csv file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LcM07zRtDX8"},"outputs":[],"source":["# Assuming optimizer is your BayesianOptimization object\n","hyperparameters = optimizer.X  # Matrix of hyperparameters\n","objective_values = optimizer.Y  # Array of objective function values\n","\n","# Define the header for the CSV file\n","header = ['learning_rate', 'num_units',\n","          'dropout_rate', 'batch_size', 'objective_value']\n","\n","# Open the CSV file in write mode\n","with open('optimization_report.csv', 'w', newline='') as file:\n","    writer = csv.writer(file)\n","\n","    # Write the header\n","    writer.writerow(header)\n","\n","    # Write the hyperparameters and objective values\n","    for i in range(len(objective_values)):\n","        # Combine hyperparameters and objective value into a single row\n","        row = list(hyperparameters[i]) + [objective_values[i]]\n","        writer.writerow(row)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPpIvIyyTtcj6IdtO1WglS6","gpuType":"T4","mount_file_id":"1BoLoDRNip_WQWlp5b_YI13rGSXgN9lNb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
