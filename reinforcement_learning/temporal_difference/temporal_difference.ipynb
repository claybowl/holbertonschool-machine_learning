{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Performs the Monte Carlo algorithm to update the value function based on the policy.\n",
    "\n",
    "    Parameters:\n",
    "    env: The environment.\n",
    "    V: The value function.\n",
    "    policy: The policy.\n",
    "    episodes: The number of episodes.\n",
    "    max_steps: The maximum number of steps per episode.\n",
    "    alpha: The learning rate.\n",
    "    gamma: The discount factor.\n",
    "\n",
    "    Returns:\n",
    "    The updated value function.\n",
    "    \"\"\"\n",
    "    # Creates dictionary with 'n' keys corresponding to a state. Value\n",
    "    # for each key will be empty list, used later for storing returns from\n",
    "    # Monte Carlo algorithm.\n",
    "    returns = {s: [] for s in range(env.observation_space.n)}\n",
    "\n",
    "    # Resets environment at beginning of each episode and initializes and empty\n",
    "    # list to store episode-related data\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        current_episode = []\n",
    "\n",
    "        # Executes one episode in environment, following policy to sle\n",
    "        for step in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            current_episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        for state, _, reward in reversed(current_episode):\n",
    "            G = gamma * G + reward\n",
    "            returns[state].append(G)\n",
    "            V[state] = (1 - alpha) * V[state] + alpha * G\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-0 Main File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64') \n",
    "np.set_printoptions(precision=2)\n",
    "env.seed(0)\n",
    "print(monte_carlo(env, V, policy).reshape((8, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Performs the TD(λ) algorithm to update the value function based on the policy.\n",
    "\n",
    "    Parameters:\n",
    "    env: The environment.\n",
    "    V: The value function.\n",
    "    policy: The policy.\n",
    "    lambtha: The trace decay parameter.\n",
    "    episodes: The number of episodes.\n",
    "    max_steps: The maximum number of steps per episode.\n",
    "    alpha: The learning rate.\n",
    "    gamma: The discount factor.\n",
    "\n",
    "    Returns:\n",
    "    The updated value function.\n",
    "    \"\"\"\n",
    "    # Initialize eligibility traces\n",
    "    E = np.zeros_like(V)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take an action, observe the reward and next state\n",
    "            action = policy[state]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Compute the TD error\n",
    "            td_error = reward + gamma * V[next_state] - V[state]\n",
    "\n",
    "            # Update eligibility trace\n",
    "            E[state] += 1\n",
    "\n",
    "            # Update V and E\n",
    "            V += alpha * td_error * E\n",
    "            E *= gamma * lambtha\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1 Main File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64') \n",
    "np.set_printoptions(precision=2)\n",
    "env.seed(0)\n",
    "print(monte_carlo(env, V, policy).reshape((8, 8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"\n",
    "    Performs the SARSA(λ) algorithm to update the action-value function based on the policy.\n",
    "\n",
    "    Parameters:\n",
    "    env: The environment.\n",
    "    Q: The action-value function.\n",
    "    lambtha: The trace decay parameter.\n",
    "    episodes: The number of episodes.\n",
    "    max_steps: The maximum number of steps per episode.\n",
    "    alpha: The learning rate.\n",
    "    gamma: The discount factor.\n",
    "    epsilon: The initial epsilon for the epsilon-greedy policy.\n",
    "    min_epsilon: The minimum epsilon.\n",
    "    epsilon_decay: The decay rate for epsilon.\n",
    "\n",
    "    Returns:\n",
    "    The updated action-value function.\n",
    "    \"\"\"\n",
    "    # Initialize eligibitility traces\n",
    "    E = np.zeros_like(Q)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        action = np.argmax(Q[state] + np.random.randn(1, env.action_space.n) * (1. / (episode + 1)))\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take an action, observe the reward and next state\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Choose next action using epsilon-greedy policy\n",
    "            next_action = np.argmax(Q[next_state] + np.random.randn(1, env.action_space.n) * (epsilon))\n",
    "\n",
    "            # Compute the TD error\n",
    "            td_error = reward + gamma * Q[next_state, next_action] - Q[state, action]\n",
    "\n",
    "            # Update eligibility trace\n",
    "            E[state, action] += 1\n",
    "\n",
    "            # Update Q and E\n",
    "            Q += alpha * td_error * E\n",
    "            E *= gamma * lambtha\n",
    "\n",
    "            # Update state and action\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, min(epsilon, 1.0 - epsilon_decay * (episode / episodes)))\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2 Main File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.7642e-03  2.6942e-03  2.5738e-03  2.5432e-03]\n",
      " [ 2.3518e-03  4.2909e-03  3.1204e-03  2.3354e-03]\n",
      " [ 3.7641e-03  5.2417e-03  2.9398e-03  3.9231e-03]\n",
      " [ 3.8798e-03  3.8118e-03  4.1403e-03  1.0694e-02]\n",
      " [ 7.1160e-03  1.4607e-02  1.4131e-02  8.1125e-03]\n",
      " [ 7.5630e-03  2.7302e-02  1.9421e-02  1.7322e-02]\n",
      " [ 4.2409e-02  1.7769e-02  3.1961e-02  1.1431e-02]\n",
      " [ 9.4788e-03  1.0871e-02  1.1786e-02  2.5303e-02]\n",
      " [ 3.5272e-03  2.8264e-03  4.1816e-03  3.6653e-03]\n",
      " [ 2.0944e-03  3.4671e-03  2.7354e-03  6.7579e-03]\n",
      " [ 3.1059e-03  9.8110e-03  2.2333e-03  2.7272e-03]\n",
      " [ 1.1634e-03  3.2468e-03  5.5223e-03  4.2952e-03]\n",
      " [ 1.3937e-02  1.3684e-02  8.0174e-03  6.3906e-03]\n",
      " [ 2.2853e-02  1.2878e-02  9.0955e-03  1.1868e-02]\n",
      " [ 3.0123e-02  1.6305e-02  2.0007e-02  1.4396e-02]\n",
      " [ 1.6091e-02  1.7035e-02  2.0579e-02  1.1247e-02]\n",
      " [ 2.3093e-03  1.7324e-03  1.6756e-03  2.0751e-03]\n",
      " [ 2.3237e-03  2.6678e-03  2.7045e-03  1.9742e-03]\n",
      " [ 1.9408e-03  2.4748e-03  4.4871e-03  2.4119e-03]\n",
      " [-5.4027e-04 -5.2404e-04 -5.2673e-04 -5.1566e-04]\n",
      " [ 2.2340e-02  4.8941e-03  7.3475e-03  9.9647e-03]\n",
      " [ 7.6128e-03  2.9867e-02  4.4468e-03  1.2934e-02]\n",
      " [ 1.9585e-02  2.1501e-02  4.8331e-02  2.0660e-02]\n",
      " [ 5.7927e-02  5.5325e-02  2.9672e-02  2.1590e-02]\n",
      " [ 2.2796e-03  1.2757e-03  2.1793e-03  1.6645e-03]\n",
      " [ 1.8488e-03  1.2160e-03  3.1211e-03  1.0094e-03]\n",
      " [ 4.5287e-03  7.2438e-04  1.2276e-03  1.5209e-03]\n",
      " [ 1.3758e-04  3.1330e-03  1.7235e-03  5.2713e-03]\n",
      " [ 2.2067e-02  1.5642e-02  1.3480e-02  3.2856e-03]\n",
      " [-1.4968e-03 -1.4532e-03 -1.4662e-03 -1.0499e-03]\n",
      " [ 2.6854e-02  3.2235e-02  2.9190e-02  5.9176e-02]\n",
      " [ 1.1248e-01  9.5249e-02  1.1596e-01  8.7829e-02]\n",
      " [ 2.2511e-03  1.1155e-03  2.0595e-03  1.5772e-03]\n",
      " [ 1.3713e-03  8.7360e-04  3.0321e-04  1.3551e-03]\n",
      " [ 6.1882e-04  5.1253e-05  1.2012e-04  9.1957e-04]\n",
      " [-6.0774e-04 -6.2382e-04 -7.2257e-04 -5.1576e-04]\n",
      " [ 7.1922e-02  6.6952e-02  9.4489e-02  3.4766e-02]\n",
      " [ 1.6353e-01  2.2292e-01  1.0514e-01  1.1700e-01]\n",
      " [ 3.2307e-02  1.2834e-01  2.7558e-02  9.1056e-02]\n",
      " [ 9.5074e-02  1.5809e-01  1.6659e-01  4.8979e-02]\n",
      " [ 3.9266e-03  9.2773e-04  3.6776e-03  1.2442e-03]\n",
      " [ 7.1927e-04  7.6979e-04  7.8555e-04  7.2651e-04]\n",
      " [ 5.1912e-04  5.1707e-04  5.3439e-04  5.3051e-04]\n",
      " [ 1.4577e-01  4.4693e-02  3.5321e-01  1.8974e-01]\n",
      " [ 1.5939e-01  2.3051e-01  3.1179e-01  8.3100e-02]\n",
      " [ 1.3345e-01  3.3364e-01  2.9740e-01  1.3627e-01]\n",
      " [-4.1956e-04 -9.8898e-04 -1.6598e-03 -1.4273e-03]\n",
      " [ 2.4951e-01  2.7240e-01  3.1973e-01  2.7536e-01]\n",
      " [ 1.8249e-02  1.5212e-02  1.0162e-02  2.9693e-03]\n",
      " [-1.1539e-04 -1.1333e-04 -1.3537e-04 -1.0795e-04]\n",
      " [ 2.5252e-01  4.6136e-01  2.8686e-01  1.4574e-01]\n",
      " [ 2.3793e-02  1.1214e-01  4.5630e-01  4.3474e-01]\n",
      " [ 8.3949e-02  8.8648e-02  8.7559e-02  8.6060e-02]\n",
      " [ 5.2816e-01  2.5581e-01  4.4771e-02  5.6198e-01]\n",
      " [ 9.9995e-02  9.6485e-02  9.7882e-02  9.9891e-02]\n",
      " [ 3.9458e-01  6.9812e-01  6.1576e-01  4.7493e-01]\n",
      " [ 2.5764e-02  5.1570e-02  5.0944e-02  3.6603e-02]\n",
      " [ 6.6664e-02  2.0588e-01  6.4209e-02  9.2616e-02]\n",
      " [ 2.3777e-01  2.9948e-02  1.8813e-01  2.2073e-01]\n",
      " [ 1.0247e-01  1.0599e-01  1.0923e-01  1.0510e-01]\n",
      " [ 8.6386e-01  1.1753e-01  5.1738e-01  1.3207e-01]\n",
      " [ 6.6977e-01  3.7059e-01  5.3020e-01  1.7878e-01]\n",
      " [ 1.4485e-01  4.8806e-01  3.5561e-01  8.5987e-01]\n",
      " [ 2.6164e-01  2.7326e-01  2.6610e-01  2.6938e-01]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "Q = np.random.uniform(size=(64, 4))\n",
    "np.set_printoptions(precision=4)\n",
    "print(sarsa_lambtha(env, Q, 0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepQ-3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
