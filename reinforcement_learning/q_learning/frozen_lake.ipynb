{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\clayb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.25.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
      "[notice] To update, run: C:\\Users\\clayb\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\clayb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\clayb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\clayb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\clayb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
      "[notice] To update, run: C:\\Users\\clayb\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frozen_lake(desc=None, map_name=None, is_slippery=False):\n",
    "    \"\"\"\n",
    "    Load the FrozenLake environment from OpenAI's gym.\n",
    "\n",
    "    Parameters:\n",
    "    desc -- None or a list of lists containing a custom\n",
    "    description of the map to load for the environment\n",
    "    map_name -- None or a string containing the pre-made map to load\n",
    "    is_slippery -- a boolean to determine if the ice is slippery\n",
    "\n",
    "    If both desc and map_name are None, the environment will\n",
    "    load a randomly generated 8x8 map.\n",
    "\n",
    "    Returns:\n",
    "    The FrozenLake environment.\n",
    "    \"\"\"\n",
    "    if desc is None and map_name is None:\n",
    "        desc = generate_random_map(size=8)\n",
    "\n",
    "    return gym.make('FrozenLake-v1', desc=desc,\n",
    "                    map_name=map_name, is_slippery=is_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F' b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'F' b'F' b'H' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'G']]\n",
      "[(1.0, 0, 0.0, False)]\n",
      "[[b'S' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'H' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'G']]\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 8, 0.0, True)]\n",
      "[[b'S' b'F' b'F']\n",
      " [b'F' b'H' b'H']\n",
      " [b'F' b'F' b'G']]\n",
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "env = load_frozen_lake()\n",
    "print(env.desc)\n",
    "print(env.P[0][0])\n",
    "env = load_frozen_lake(is_slippery=True)\n",
    "print(env.desc)\n",
    "print(env.P[0][0])\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "print(env.desc)\n",
    "env = load_frozen_lake(map_name='4x4')\n",
    "print(env.desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_init(env):\n",
    "    \"\"\"Initialize Q-table to zeros.\"\"\"\n",
    "    return np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "(64, 4)\n",
      "(9, 4)\n",
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "env = load_frozen_lake()\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "env = load_frozen_lake(is_slippery=True)\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "env = load_frozen_lake(map_name='4x4')\n",
    "Q = q_init(env)\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"Epsilon-greedy policy.\"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = env.action_space.sample()  # Explore action space\n",
    "    else:\n",
    "        action = np.argmax(Q[state])  # Exploit learned values\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "Q[7] = np.array([0.5, 0.7, 1, -1])\n",
    "np.random.seed(0)\n",
    "print(epsilon_greedy(Q, 7, 0.5))\n",
    "np.random.seed(1)\n",
    "print(epsilon_greedy(Q, 7, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, Q, episodes=5000, max_steps=100,\n",
    "          alpha=0.1, gamma=0.99, epsilon=1,\n",
    "          min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"Train agent to learn Q-values.\"\"\"\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # initialize new episode parameters\n",
    "        state, _ = env.reset()\n",
    "        ### done = False\n",
    "        ep_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # exploration/exploitation trade-off\n",
    "            action = epsilon_greedy(Q, state, epsilon)\n",
    "\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "            # print(\"Episode: \", episode, \"Step: \", step)\n",
    "            # print(\"New State: \", new_state, \"Reward: \", reward, \"Terminated: \", terminated)\n",
    "\n",
    "            if reward == 1.0 and done == True:\n",
    "                rewards += reward\n",
    "            if reward == 0.0 and done == True:\n",
    "                rewards -= 1\n",
    "                reward = -1\n",
    "            if reward == 0.0 and step + 1 == max_steps:\n",
    "                rewards += 0\n",
    "\n",
    "            # Update Q-table for Q(s,a)\n",
    "            Q[state, action] = Q[state, action] * (1 - alpha) + \\\n",
    "            alpha * (reward + gamma * np.max(Q[new_state, :]))\n",
    "\n",
    "            state = new_state\n",
    "            total_rewards += reward \n",
    "\n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "        # Exploration rate decay\n",
    "    epsilon = min_epsilon + \\\n",
    "    (max_steps - min_epsilon) * np.exp(-epsilon_decay*episode)\n",
    "\n",
    "    total_rewards.append(rewards)\n",
    "\n",
    "\n",
    "    return Q, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.5  0.7  1.  -1. ]\n",
      " [ 0.   0.   0.   0. ]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m Q \u001b[39m=\u001b[39m q_init(env)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(Q)\n\u001b[1;32m----> 7\u001b[0m Q, total_rewards  \u001b[39m=\u001b[39m train(env, Q)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(Q)\n\u001b[0;32m      9\u001b[0m split_rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msplit(np\u001b[39m.\u001b[39marray(total_rewards), \u001b[39m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, Q, episodes, max_steps, alpha, gamma, epsilon, min_epsilon, epsilon_decay)\u001b[0m\n\u001b[0;32m     30\u001b[0m Q[state, action] \u001b[39m=\u001b[39m Q[state, action] \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha) \u001b[39m+\u001b[39m \\\n\u001b[0;32m     31\u001b[0m alpha \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(Q[new_state, :]))\n\u001b[0;32m     33\u001b[0m state \u001b[39m=\u001b[39m new_state\n\u001b[1;32m---> 34\u001b[0m total_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward \n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m done \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "print(Q)\n",
    "np.random.seed(0)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "print(Q)\n",
    "Q, total_rewards  = train(env, Q)\n",
    "print(Q)\n",
    "split_rewards = np.split(np.array(total_rewards), 10)\n",
    "for i, rewards in enumerate(split_rewards):\n",
    "    print((i+1) * 500, ':', np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of env.step(action): (0, 0.0, False, False, {'prob': 1.0})\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and Q-table as you did before\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "\n",
    "# Reset the environment to get the initial state\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Choose an action (for testing purposes, let's choose 0)\n",
    "action = 0\n",
    "\n",
    "# Now, try running the step function and print its output\n",
    "output = env.step(action)\n",
    "print(\"Output of env.step(action):\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[243], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m env \u001b[39m=\u001b[39m load_frozen_lake(desc\u001b[39m=\u001b[39mdesc)\n\u001b[0;32m      4\u001b[0m Q \u001b[39m=\u001b[39m q_init(env)\n\u001b[1;32m----> 6\u001b[0m Q, total_rewards  \u001b[39m=\u001b[39m train(env, Q)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(Q)\n\u001b[0;32m      8\u001b[0m split_rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msplit(np\u001b[39m.\u001b[39marray(total_rewards), \u001b[39m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[236], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, Q, episodes, max_steps, alpha, gamma, epsilon, min_epsilon, epsilon_decay)\u001b[0m\n\u001b[0;32m     27\u001b[0m     rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[39m# Update Q-table for Q(s,a)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m Q[state, action] \u001b[39m=\u001b[39m Q[state, action] \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha) \u001b[39m+\u001b[39m \\\n\u001b[0;32m     31\u001b[0m alpha \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(Q[new_state, :]))\n\u001b[0;32m     33\u001b[0m state \u001b[39m=\u001b[39m new_state\n\u001b[0;32m     34\u001b[0m total_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward \n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "\n",
    "Q, total_rewards  = train(env, Q)\n",
    "print(Q)\n",
    "split_rewards = np.split(np.array(total_rewards), 10)\n",
    "for i, rewards in enumerate(split_rewards):\n",
    "    print((i+1) * 500, ':', np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, Q, max_steps=100):\n",
    "    \"\"\"Play agent with learned Q-values.\"\"\"\n",
    "    state = env.reset()\n",
    "    total_rewards = 0\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        state = new_state\n",
    "        total_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(Q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
