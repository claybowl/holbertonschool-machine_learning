Policy Gradients

    Master
    By: Alexa Orrico, Software Engineer at Holberton School
    Weight: 1
    Manual QA review must be done (request it when you are done with the project)

In this project, you will implement your own Policy Gradient in your loop of reinforcement learning (by using the Monte-Carlo policy gradient algorithm - also called REINFORCE).
Resources

Read or watch:

    How Policy Gradient Reinforcement Learning Works
    Policy Gradients in a Nutshell
    RL Course by David Silver - Lecture 7: Policy Gradient Methods
    Reinforcement Learning 6: Policy Gradients and Actor Critics
    Policy Gradient Algorithms

Learning Objectives

    What is Policy?
    How to calculate a Policy Gradient?
    What and how to use a Monte-Carlo policy gradient?

Requirements
General

    Allowed editors: vi, vim, emacs
    All your files will be interpreted/compiled on Ubuntu 16.04 LTS using python3 (version 3.5)
    Your files will be executed with numpy (version 1.15), and gym (version 0.7)
    All your files should end with a new line
    The first line of all your files should be exactly #!/usr/bin/env python3
    A README.md file, at the root of the folder of the project, is mandatory
    Your code should use the pycodestyle style (version 2.4)
    All your modules should have documentation (python3 -c 'print(__import__("my_module").__doc__)')
    All your classes should have documentation (python3 -c 'print(__import__("my_module").MyClass.__doc__)')
    All your functions (inside and outside a class) should have documentation (python3 -c 'print(__import__("my_module").my_function.__doc__)' and python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)')
    All your files must be executable
    Your code should use the minimum number of operations
