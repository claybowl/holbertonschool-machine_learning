{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "6ac5ca9b-7759-5bd7-9cb1-0543a010e525",
        "openai_ephemeral_user_id": "1dbf38a2-c23d-516c-8adc-70cf115f54c7",
        "openai_subdivision1_iso_code": "US-OK"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "ab88c428-34d9-478f-933e-bb9128b062aa"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "b4be78b0-a2a7-494e-a249-93feac31c57f",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    },
    {
      "id": "aca1c8d9-5b0c-4997-be0a-9033aaf22f5d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Installing the required packages\n!pip install -q numpy==1.18 tensorflow==2.3 tensorflow-hub transformers\n\n# Importing necessary libraries\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom transformers import BertTokenizer\n\n# Checking the installed versions\nprint('Tensorflow version:', tf.__version__)\nprint('Numpy version:', np.__version__)",
      "outputs": []
    },
    {
      "id": "bba2cb44-dc59-452c-980b-4a05d69d911c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Loading the BERT model for question answering\nbert_qa_model_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3'\nbert_qa_model = hub.load(bert_qa_model_url)\n\n# Loading the pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\ndef question_answer(question, reference):\n    \"\"\"Finds a snippet of text within a reference document to answer a question.\n\n    Args:\n        question (str): The question to answer.\n        reference (str): The reference document from which to find the answer.\n\n    Returns:\n        str: The answer as a string or None if no answer is found.\n    \"\"\"\n    # TODO: Implement the function to find the answer using the BERT model and tokenizer\n    pass",
      "outputs": []
    },
    {
      "id": "3eb204c7-cfaa-4288-bba0-dc9da7911494",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "def preprocess_input(question, reference, tokenizer):\n    \"\"\"Preprocesses the question and reference text for the BERT model.\n\n    Args:\n        question (str): The question to answer.\n        reference (str): The reference document from which to find the answer.\n        tokenizer (BertTokenizer): The BERT tokenizer.\n\n    Returns:\n        dict: A dictionary containing the input IDs, attention masks, and token type IDs.\n    \"\"\"\n    # Tokenize the question and reference\n    inputs = tokenizer(question, reference, return_tensors='tf', max_length=512, truncation=True)\n\n    # Create input IDs, attention masks, and token type IDs\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    token_type_ids = inputs['token_type_ids']\n\n    # Prepare the model input\n    model_input = {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids\n    }\n\n    return model_input",
      "outputs": []
    },
    {
      "id": "a37baeeb-c1ae-4969-8464-a8f394e97d63",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "def question_answer(question, reference):\n    \"\"\"Finds a snippet of text within a reference document to answer a question.\n\n    Args:\n        question (str): The question to answer.\n        reference (str): The reference document from which to find the answer.\n\n    Returns:\n        str: The answer as a string or None if no answer is found.\n    \"\"\"\n    # Preprocess the input\n    model_input = preprocess_input(question, reference, tokenizer)\n\n    # Run the BERT model\n    outputs = bert_qa_model(model_input)\n    start_logits, end_logits = outputs['start_logits'], outputs['end_logits']\n\n    # Find the answer span\n    start_token_idx = tf.argmax(start_logits, axis=-1)[0]\n    end_token_idx = tf.argmax(end_logits, axis=-1)[0]\n\n    # Extract the answer text\n    input_ids = model_input['input_ids'].numpy()[0]\n    answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[start_token_idx:end_token_idx + 1])\n    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n\n    return answer if answer else None",
      "outputs": []
    },
    {
      "id": "22d2a114-14cc-4826-bb8f-e9b2e8ec509b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "def answer_loop(reference):\n    while True:\n        question = input(\"Q: \").strip().lower()\n        if question in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n            print(\"A: Goodbye\")\n            break\n        else:\n            answer = question_answer(question, reference)\n            if answer:\n                print(\"A:\", answer)\n            else:\n                print(\"A: Sorry, I do not understand your question.\")",
      "outputs": []
    },
    {
      "id": "5f2dcba1-c895-406e-a312-38bc59e63e5c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Installing the Universal Sentence Encoder\n!pip install -q tensorflow-text\n\n# Importing necessary libraries\nimport tensorflow_text as text\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Loading the Universal Sentence Encoder\nuse_model_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'\nuse_model = hub.load(use_model_url)\n\ndef semantic_search(corpus_path, sentence):\n    \"\"\"Performs semantic search on a corpus of documents.\n\n    Args:\n        corpus_path (str): The path to the corpus of reference documents.\n        sentence (str): The sentence from which to perform semantic search.\n\n    Returns:\n        str: The reference text of the document most similar to the sentence.\n    \"\"\"\n    # TODO: Read the corpus, encode the sentence and documents, compute similarity, and find the most similar document\n    pass",
      "outputs": []
    },
    {
      "id": "dc3a9c48-5876-4895-b79a-33a604d810b4",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "def semantic_search(corpus_path, sentence):\n    # Read the corpus\n    corpus = []\n    with open(corpus_path, 'r') as file:\n        corpus = file.readlines()\n\n    # Encode the sentence and documents\n    sentence_embedding = use_model([sentence])\n    document_embeddings = use_model(corpus)\n\n    # Compute cosine similarity\n    similarities = cosine_similarity(sentence_embedding, document_embeddings)\n\n    # Find the most similar document\n    most_similar_idx = np.argmax(similarities)\n    most_similar_document = corpus[most_similar_idx]\n\n    return most_similar_document",
      "outputs": []
    },
    {
      "id": "9386de31-33e4-4f60-b1be-39b4a780ca61",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "def question_answer(corpus_path):\n    while True:\n        question = input(\"Q: \").strip().lower()\n        if question in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n            print(\"A: Goodbye\")\n            break\n        else:\n            # Perform semantic search to find the most relevant reference text\n            reference = semantic_search(corpus_path, question)\n\n            # Find the specific answer within the reference text\n            answer = question_answer(question, reference)\n\n            if answer:\n                print(\"A:\", answer)\n            else:\n                print(\"A: Sorry, I do not understand your question.\")",
      "outputs": []
    }
  ]
}