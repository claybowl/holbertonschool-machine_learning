{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca1c8d9-5b0c-4997-be0a-9033aaf22f5d",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Installing the required packages\n",
        "!pip install -q numpy==1.18 tensorflow==2.3 tensorflow-hub transformers\n",
        "\n",
        "# Importing necessary libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Checking the installed versions\n",
        "print('Tensorflow version:', tf.__version__)\n",
        "print('Numpy version:', np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba2cb44-dc59-452c-980b-4a05d69d911c",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Loading the BERT model for question answering\n",
        "bert_qa_model_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3'\n",
        "bert_qa_model = hub.load(bert_qa_model_url)\n",
        "\n",
        "# Loading the pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb204c7-cfaa-4288-bba0-dc9da7911494",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "def preprocess_input(question, reference, tokenizer):\n",
        "    \"\"\"Preprocesses the question and reference text for the BERT model.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to answer.\n",
        "        reference (str): The reference document from which to find the answer.\n",
        "        tokenizer (BertTokenizer): The BERT tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the input IDs, attention masks, and token type IDs.\n",
        "    \"\"\"\n",
        "    # Tokenize the question and reference\n",
        "    inputs = tokenizer(question, reference, return_tensors='tf', max_length=512, truncation=True)\n",
        "\n",
        "    # Create input IDs, attention masks, and token type IDs\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "    # Prepare the model input\n",
        "    model_input = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'token_type_ids': token_type_ids\n",
        "    }\n",
        "\n",
        "    return model_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37baeeb-c1ae-4969-8464-a8f394e97d63",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "def question_answer(question, reference):\n",
        "    \"\"\"Finds a snippet of text within a reference document to answer a question.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to answer.\n",
        "        reference (str): The reference document from which to find the answer.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer as a string or None if no answer is found.\n",
        "    \"\"\"\n",
        "    # Preprocess the input\n",
        "    model_input = preprocess_input(question, reference, tokenizer)\n",
        "\n",
        "    # Run the BERT model\n",
        "    outputs = bert_qa_model(model_input)\n",
        "    start_logits, end_logits = outputs['start_logits'], outputs['end_logits']\n",
        "\n",
        "    # Find the answer span\n",
        "    start_token_idx = tf.argmax(start_logits, axis=-1)[0]\n",
        "    end_token_idx = tf.argmax(end_logits, axis=-1)[0]\n",
        "\n",
        "    # Extract the answer text\n",
        "    input_ids = model_input['input_ids'].numpy()[0]\n",
        "    answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[start_token_idx:end_token_idx + 1])\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "    return answer if answer else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d2a114-14cc-4826-bb8f-e9b2e8ec509b",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "def answer_loop(reference):\n",
        "    while True:\n",
        "        question = input(\"Q: \").strip().lower()\n",
        "        if question in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "            print(\"A: Goodbye\")\n",
        "            break\n",
        "        else:\n",
        "            answer = question_answer(question, reference)\n",
        "            if answer:\n",
        "                print(\"A:\", answer)\n",
        "            else:\n",
        "                print(\"A: Sorry, I do not understand your question.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2dcba1-c895-406e-a312-38bc59e63e5c",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Installing the Universal Sentence Encoder\n",
        "!pip install -q tensorflow-text\n",
        "\n",
        "# Importing necessary libraries\n",
        "import tensorflow_text as text\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Loading the Universal Sentence Encoder\n",
        "use_model_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n",
        "use_model = hub.load(use_model_url)\n",
        "\n",
        "def semantic_search(corpus_path, sentence):\n",
        "    \"\"\"Performs semantic search on a corpus of documents.\n",
        "\n",
        "    Args:\n",
        "        corpus_path (str): The path to the corpus of reference documents.\n",
        "        sentence (str): The sentence from which to perform semantic search.\n",
        "\n",
        "    Returns:\n",
        "        str: The reference text of the document most similar to the sentence.\n",
        "    \"\"\"\n",
        "    # TODO: Read the corpus, encode the sentence and documents, compute similarity, and find the most similar document\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3a9c48-5876-4895-b79a-33a604d810b4",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "def semantic_search(corpus_path, sentence):\n",
        "    # Read the corpus\n",
        "    corpus = []\n",
        "    with open(corpus_path, 'r') as file:\n",
        "        corpus = file.readlines()\n",
        "\n",
        "    # Encode the sentence and documents\n",
        "    sentence_embedding = use_model([sentence])\n",
        "    document_embeddings = use_model(corpus)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarities = cosine_similarity(sentence_embedding, document_embeddings)\n",
        "\n",
        "    # Find the most similar document\n",
        "    most_similar_idx = np.argmax(similarities)\n",
        "    most_similar_document = corpus[most_similar_idx]\n",
        "\n",
        "    return most_similar_document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9386de31-33e4-4f60-b1be-39b4a780ca61",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "def question_answer(corpus_path):\n",
        "    while True:\n",
        "        question = input(\"Q: \").strip().lower()\n",
        "        if question in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "            print(\"A: Goodbye\")\n",
        "            break\n",
        "        else:\n",
        "            # Perform semantic search to find the most relevant reference text\n",
        "            reference = semantic_search(corpus_path, question)\n",
        "\n",
        "            # Find the specific answer within the reference text\n",
        "            answer = question_answer(question, reference)\n",
        "\n",
        "            if answer:\n",
        "                print(\"A:\", answer)\n",
        "            else:\n",
        "                print(\"A: Sorry, I do not understand your question.\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "ab88c428-34d9-478f-933e-bb9128b062aa"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "6ac5ca9b-7759-5bd7-9cb1-0543a010e525",
        "openai_ephemeral_user_id": "1dbf38a2-c23d-516c-8adc-70cf115f54c7",
        "openai_subdivision1_iso_code": "US-OK"
      }
    },
    "selected_hardware_size": "small"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
