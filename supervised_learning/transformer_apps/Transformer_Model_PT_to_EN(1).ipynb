{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "53f17414-3639-5a61-9946-06b291ac50d1",
        "openai_ephemeral_user_id": "971e7c30-390f-5fd1-9fc5-2fe54a12e76f",
        "openai_subdivision1_iso_code": "US-OK"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "28ac74ab-c05f-410d-a8ba-b14db6476d99"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "eb6a9793-c178-4c9e-961e-224b14eb3415",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "fc237e37-ca7f-4de8-be06-2a4aa2211748"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:17:57.741829+00:00",
          "start_time": "2023-08-22T06:17:07.612318+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install tensorflow tensorflow_datasets\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nclass Dataset:\n    def __init__(self):\n        # Load the ted_hrlr_translate/pt_to_en tf.data.Dataset train split\n        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n        # Load the ted_hrlr_translate/pt_to_en tf.data.Dataset validate split\n        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n        # Create Portuguese and English tokenizers from the training set\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)",
      "outputs": []
    },
    {
      "id": "011d9987-7aa4-4095-87bd-af453d8e4b6b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "adfd09e8-9e9f-47fd-9286-9c48d48f7c8e"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:17:57.906647+00:00",
          "start_time": "2023-08-22T06:17:57.749832+00:00"
        }
      },
      "execution_count": null,
      "source": "import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nclass Dataset:\n    def __init__(self):\n        # Load the ted_hrlr_translate/pt_to_en tf.data.Dataset train split\n        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n        # Load the ted_hrlr_translate/pt_to_en tf.data.Dataset validate split\n        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n        # Create Portuguese and English tokenizers from the training set\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)",
      "outputs": []
    },
    {
      "id": "a0463c4b-9bc8-4bc7-8c31-5a7b543b5953",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "445676b8-6bad-4a91-9999-c359905b8908"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:18:04.054964+00:00",
          "start_time": "2023-08-22T06:17:57.915502+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install tensorflow-datasets\nfrom tensorflow_datasets.core.features.text import SubwordTextEncoder\n\nclass Dataset:\n    def __init__(self):\n        # Same as previous code\n        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n\n    def tokenize_dataset(self, data):\n        \"\"\" Creates sub-word tokenizers for our dataset \"\"\"\n        # Create a list of Portuguese and English sentences\n        pt_sentences = [pt.numpy() for pt, en in data]\n        en_sentences = [en.numpy() for pt, en in data]\n        # Create Portuguese tokenizer\n        tokenizer_pt = SubwordTextEncoder.build_from_corpus((sentence for sentence in pt_sentences), target_vocab_size=2**15)\n        # Create English tokenizer\n        tokenizer_en = SubwordTextEncoder.build_from_corpus((sentence for sentence in en_sentences), target_vocab_size=2**15)\n        return tokenizer_pt, tokenizer_en",
      "outputs": []
    },
    {
      "id": "3b11b8e9-cb5e-434d-81ba-db8fa9541532",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "35e5c038-373d-4a96-bf1c-cec7e205a93a"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:18:52.927645+00:00",
          "start_time": "2023-08-22T06:18:52.732224+00:00"
        }
      },
      "execution_count": null,
      "source": "import tensorflow_text as text\n\nclass Dataset:\n    def __init__(self):\n        # Same as previous code\n        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n\n    def tokenize_dataset(self, data):\n        \"\"\" Creates sub-word tokenizers for our dataset \"\"\"\n        # Create a list of Portuguese and English sentences\n        pt_sentences = [pt.numpy() for pt, en in data]\n        en_sentences = [en.numpy() for pt, en in data]\n        # Create Portuguese tokenizer\n        tokenizer_pt = text.SubwordTextEncoder.build_from_corpus((sentence for sentence in pt_sentences), target_vocab_size=2**15)\n        # Create English tokenizer\n        tokenizer_en = text.SubwordTextEncoder.build_from_corpus((sentence for sentence in en_sentences), target_vocab_size=2**15)\n        return tokenizer_pt, tokenizer_en",
      "outputs": []
    },
    {
      "id": "5cd81658-dec7-4d41-89a6-4e158c8bde24",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8caf5ae9-6c81-431f-84c2-a0cba9477a43"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:18:57.362743+00:00",
          "start_time": "2023-08-22T06:18:57.199055+00:00"
        }
      },
      "execution_count": null,
      "source": "import numpy as np\n\nclass Dataset:\n    def __init__(self):\n        # Same as previous code\n        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n\n    def tokenize_dataset(self, data):\n        # Same as previous code\n        pt_sentences = [pt.numpy() for pt, en in data]\n        en_sentences = [en.numpy() for pt, en in data]\n        tokenizer_pt = text.SubwordTextEncoder.build_from_corpus((sentence for sentence in pt_sentences), target_vocab_size=2**15)\n        tokenizer_en = text.SubwordTextEncoder.build_from_corpus((sentence for sentence in en_sentences), target_vocab_size=2**15)\n        return tokenizer_pt, tokenizer_en\n\n    def encode(self, pt, en):\n        \"\"\" Encodes a translation into tokens \"\"\"\n        # Add start and end tokens to Portuguese and English sentences\n        pt_tokens = [self.tokenizer_pt.vocab_size] + self.tokenizer_pt.encode(pt.numpy()) + [self.tokenizer_pt.vocab_size + 1]\n        en_tokens = [self.tokenizer_en.vocab_size] + self.tokenizer_en.encode(en.numpy()) + [self.tokenizer_en.vocab_size + 1]\n        return np.array(pt_tokens), np.array(en_tokens)",
      "outputs": []
    },
    {
      "id": "297fc0de-793d-4e38-886f-b55753eea9fd",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "86cb990f-de09-4def-b48e-5fdf8b45b1a6"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:01.774789+00:00",
          "start_time": "2023-08-22T06:19:01.611738+00:00"
        }
      },
      "execution_count": null,
      "source": "class Dataset:\n    def __init__(self):\n        # Load the ted_hrlr_translate/pt_to_en tf.data.Dataset train split\n        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n        # Load the ted_hrlr_translate/pt_to_en tf.data.Dataset validate split\n        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n        # Create Portuguese and English tokenizers from the training set\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n        # Tokenize the examples in data_train and data_validate\n        self.data_train = self.data_train.map(self.tf_encode)\n        self.data_valid = self.data_valid.map(self.tf_encode)\n\n    # Same as previous code\n    def tokenize_dataset(self, data):\n        # ...\n        return tokenizer_pt, tokenizer_en\n\n    def encode(self, pt, en):\n        # ...\n        return np.array(pt_tokens), np.array(en_tokens)\n\n    def tf_encode(self, pt, en):\n        \"\"\" TensorFlow wrapper for the encode instance method \"\"\"\n        pt_tokens, en_tokens = tf.py_function(func=self.encode, inp=[pt, en], Tout=[tf.int64, tf.int64])\n        # Set the shape of the pt and en return tensors\n        pt_tokens.set_shape([None])\n        en_tokens.set_shape([None])\n        return pt_tokens, en_tokens",
      "outputs": []
    },
    {
      "id": "556f3c4d-0f45-4501-9348-580f29406ea5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "502e678c-96e4-4c6e-9c08-fc60b8eae7eb"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:06.291811+00:00",
          "start_time": "2023-08-22T06:19:06.129163+00:00"
        }
      },
      "execution_count": null,
      "source": "class Dataset:\n    def __init__(self, batch_size=64, max_length=40):\n        # Load the ted_hrlr_translate/pt_to_en tf.data.Dataset train split\n        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n        # Load the ted_hrlr_translate/pt_to_en tf.data.Dataset validate split\n        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n        # Create Portuguese and English tokenizers from the training set\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n        # Tokenize the examples in data_train and data_validate\n        self.data_train = self.data_train.map(self.tf_encode)\n        self.data_valid = self.data_valid.map(self.tf_encode)\n        # Filter out translations that are longer than max_length\n        self.data_train = self.data_train.filter(lambda x, y: tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length))\n        self.data_valid = self.data_valid.filter(lambda x, y: tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length))\n        # Batch the data\n        self.data_train = self.data_train.padded_batch(batch_size, padded_shapes=([None], [None]))\n        self.data_valid = self.data_valid.padded_batch(batch_size, padded_shapes=([None], [None]))\n\n    # Same as previous code\n    def tokenize_dataset(self, data):\n        # ...\n        return tokenizer_pt, tokenizer_en\n\n    def encode(self, pt, en):\n        # ...\n        return np.array(pt_tokens), np.array(en_tokens)\n\n    def tf_encode(self, pt, en):\n        # ...\n        return pt_tokens, en_tokens",
      "outputs": []
    },
    {
      "id": "9efa3155-304f-46bc-8c8d-ef80907c5ea8",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "67eccd79-dfff-4612-b8d8-7401329afd8d"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:13.555573+00:00",
          "start_time": "2023-08-22T06:19:13.394452+00:00"
        }
      },
      "execution_count": null,
      "source": "def create_masks(inputs, target):\n    \"\"\"\n    Creates all masks for training/validation.\n    inputs: tf.Tensor of shape (batch_size, seq_len_in) containing the input sentence\n    target: tf.Tensor of shape (batch_size, seq_len_out) containing the target sentence\n    Returns: encoder_mask, combined_mask, decoder_mask\n    \"\"\"\n    # Encoder padding mask\n    encoder_mask = tf.cast(tf.math.equal(inputs, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n\n    # Decoder padding mask\n    decoder_mask = tf.cast(tf.math.equal(inputs, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n\n    # Look-ahead mask for target\n    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((target.shape[1], target.shape[1])), -1, 0)\n\n    # Decoder target padding mask\n    dec_target_padding_mask = tf.cast(tf.math.equal(target, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n\n    # Combined mask for the first attention block in the decoder\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n    return encoder_mask, combined_mask, decoder_mask",
      "outputs": []
    },
    {
      "id": "b3bf4306-02d8-4993-b09e-e5df3ca06519",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "08bae763-0138-447e-92e3-6ab10a81ec71"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:17.828651+00:00",
          "start_time": "2023-08-22T06:19:17.668847+00:00"
        }
      },
      "execution_count": null,
      "source": "def scaled_dot_product_attention(q, k, v, mask=None):\n    \"\"\"\n    Compute the scaled dot-product attention.\n    q: Query shape == (..., seq_len_q, depth)\n    k: Key shape == (..., seq_len_k, depth)\n    v: Value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k)\n    Returns: output, attention_weights\n    \"\"\"\n    # Compute the dot product between queries and keys\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n    # Scale the dot product by the square root of the depth\n    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n\n    # Apply the mask if provided\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n\n    # Compute the attention weights\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n\n    # Compute the output as a weighted sum of the values\n    output = tf.matmul(attention_weights, v)\n\n    return output, attention_weights",
      "outputs": []
    },
    {
      "id": "6dc86ae4-07a5-49e0-b9d8-f973eecbe42d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "624085ea-7651-4b4a-8049-cf8707bb0f1a"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:22.943381+00:00",
          "start_time": "2023-08-22T06:19:22.758717+00:00"
        }
      },
      "execution_count": null,
      "source": "class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # Apply scaled dot-product attention\n        output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n\n        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n        concatenated_output = tf.reshape(output, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        # Final linear layer\n        output = self.dense(concatenated_output)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights",
      "outputs": []
    },
    {
      "id": "59226f6d-d24f-4635-bd30-ccec702c321a",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e8896e40-1f7e-422d-9b6e-b4d401592611"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:27.963965+00:00",
          "start_time": "2023-08-22T06:19:27.799343+00:00"
        }
      },
      "execution_count": null,
      "source": "import tensorflow as tf\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # Apply scaled dot-product attention\n        output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n\n        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n        concatenated_output = tf.reshape(output, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        # Final linear layer\n        output = self.dense(concatenated_output)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights",
      "outputs": []
    },
    {
      "id": "409cf76e-051e-4e8d-bbe2-15ee053b374f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "58df2616-2bdf-499a-9d35-f3de7dd13a97"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:37.057910+00:00",
          "start_time": "2023-08-22T06:19:31.192776+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install tensorflow",
      "outputs": []
    },
    {
      "id": "4aabb981-99d0-4052-b98a-e691498cea47",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a46a0b52-fbff-4fb3-82fe-7151f4e91cec"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:37.258924+00:00",
          "start_time": "2023-08-22T06:19:37.100915+00:00"
        }
      },
      "execution_count": null,
      "source": "def point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    ])",
      "outputs": []
    },
    {
      "id": "f234f06a-2e2f-4efc-b2ac-21e10d5a3ceb",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e706d778-8765-45ac-aab1-01b9c5094eef"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:41.106970+00:00",
          "start_time": "2023-08-22T06:19:40.944385+00:00"
        }
      },
      "execution_count": null,
      "source": "class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2",
      "outputs": []
    },
    {
      "id": "8f247ffd-6ed9-49c4-88e5-7009c92a8743",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "26b3396d-e21d-41b4-bfbb-a9d26e5ba81c"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:44.892938+00:00",
          "start_time": "2023-08-22T06:19:44.729622+00:00"
        }
      },
      "execution_count": null,
      "source": "class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2",
      "outputs": []
    },
    {
      "id": "a01c8cd8-37ca-44ff-b079-4b6d705d36b6",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "0a8ebaec-9aec-46eb-81ba-abb81495181a"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:49.162100+00:00",
          "start_time": "2023-08-22T06:19:48.999662+00:00"
        }
      },
      "execution_count": null,
      "source": "class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        # Adding embedding and position encoding.\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)",
      "outputs": []
    },
    {
      "id": "5cc4f9f3-f64d-41eb-9265-ce1671e2fc80",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "50500a24-caee-4fcf-8906-3a39ca5d3758"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:55.362202+00:00",
          "start_time": "2023-08-22T06:19:55.198975+00:00"
        }
      },
      "execution_count": null,
      "source": "class Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n\n            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n\n        return x, attention_weights  # x.shape == (batch_size, target_seq_len, d_model)",
      "outputs": []
    },
    {
      "id": "83bbcb9f-f3ae-415d-b2f5-54597e0f7953",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "0a0f6b94-6dd2-43e9-9d32-21bef6075146"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:19:59.338177+00:00",
          "start_time": "2023-08-22T06:19:59.176138+00:00"
        }
      },
      "execution_count": null,
      "source": "class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output, attention_weights",
      "outputs": []
    },
    {
      "id": "51e6bd95-c848-420f-b110-27c9e4997a58",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8274031d-209f-4516-b11c-8d3d8e36e304"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:20:03.303171+00:00",
          "start_time": "2023-08-22T06:20:03.141415+00:00"
        }
      },
      "execution_count": null,
      "source": "# Loss Function\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n\n# Learning Rate Schedule\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)",
      "outputs": []
    },
    {
      "id": "3f45636d-3bfe-403a-9008-ee6e9c34761d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "721db3f0-d57a-4019-870d-5535f9b8a9e8"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:20:07.712863+00:00",
          "start_time": "2023-08-22T06:20:07.499221+00:00"
        }
      },
      "execution_count": null,
      "source": "# Model Parameters\nnum_layers = 4\nd_model = 128\nnum_heads = 8\ndff = 512\ninput_vocab_size = tokenizer_pt.vocab_size + 2\ntarget_vocab_size = tokenizer_en.vocab_size + 2\npe_input = 10000\npe_target = 6000\n\n# Transformer Model\ntransformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n\n# Learning Rate and Optimizer\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\n# Loss and Metrics\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')",
      "outputs": []
    },
    {
      "id": "8bf56974-5b8e-4e58-946a-ee3c39f507c6",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4e4f5ea1-da01-4fe1-99bf-8e72e4a333ae"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:20:26.956109+00:00",
          "start_time": "2023-08-22T06:20:12.923189+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating Dataset instance\ndataset = Dataset()\n\n# Model Parameters\nnum_layers = 4\nd_model = 128\nnum_heads = 8\ndff = 512\ninput_vocab_size = dataset.tokenizer_pt.vocab_size + 2\ntarget_vocab_size = dataset.tokenizer_en.vocab_size + 2\npe_input = 10000\npe_target = 6000\n\n# Transformer Model\ntransformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n\n# Learning Rate and Optimizer\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\n# Loss and Metrics\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')",
      "outputs": []
    },
    {
      "id": "7985ef59-83a6-4338-9980-2c19318c2bfc",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "c94ef664-7519-4203-8863-7a588ddee36d"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:23:23.797900+00:00",
          "start_time": "2023-08-22T06:23:23.598592+00:00"
        }
      },
      "execution_count": null,
      "source": "# Training Step Function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n\n# Training Loop\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    # inp -> Portuguese, tar -> English\n    for (batch, (inp, tar)) in enumerate(dataset.data_train):\n        train_step(inp, tar)\n\n        if batch % 50 == 0:\n            print(f'Epoch {epoch + 1}, Batch {batch}, Loss {train_loss.result()}, Accuracy {train_accuracy.result() * 100}')\n\n    print(f'Epoch {epoch + 1}, Loss {train_loss.result()}, Accuracy {train_accuracy.result() * 100}')\n    print(f'Time taken for 1 epoch: {time.time() - start} secs\\n')",
      "outputs": []
    },
    {
      "id": "b76d258f-95ea-41f5-9a2f-0c9489df994d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "018f8368-59c0-4989-942e-d8c61ea5a0dc"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:23:47.692630+00:00",
          "start_time": "2023-08-22T06:23:47.492934+00:00"
        }
      },
      "execution_count": null,
      "source": "import time\n\n# Training Loop\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    # inp -> Portuguese, tar -> English\n    for (batch, (inp, tar)) in enumerate(dataset.data_train):\n        train_step(inp, tar)\n\n        if batch % 50 == 0:\n            print(f'Epoch {epoch + 1}, Batch {batch}, Loss {train_loss.result()}, Accuracy {train_accuracy.result() * 100}')\n\n    print(f'Epoch {epoch + 1}, Loss {train_loss.result()}, Accuracy {train_accuracy.result() * 100}')\n    print(f'Time taken for 1 epoch: {time.time() - start} secs\\n')",
      "outputs": []
    },
    {
      "id": "c427601a-e734-431d-8af4-3745726fad57",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e91e87b4-478e-4e6b-b2ad-6f09c2866b60"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:23:52.272966+00:00",
          "start_time": "2023-08-22T06:23:51.895668+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating Dataset instance\ndataset = Dataset()\n\n# Model Parameters\nnum_layers = 4\nd_model = 128\nnum_heads = 8\ndff = 512\ninput_vocab_size = dataset.tokenizer_pt.vocab_size + 2\ntarget_vocab_size = dataset.tokenizer_en.vocab_size + 2\npe_input = 10000\npe_target = 6000\n\n# Transformer Model\ntransformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n\n# Learning Rate and Optimizer\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\n# Loss and Metrics\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')",
      "outputs": []
    },
    {
      "id": "c649f28b-92dc-41a2-80f7-17248abefef2",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e4ec53da-ef8f-43c2-a34f-540734656e45"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:24:00.715434+00:00",
          "start_time": "2023-08-22T06:24:00.548264+00:00"
        }
      },
      "execution_count": null,
      "source": "class Dataset:\n    def __init__(self, batch_size, max_len):\n        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n        self.data_train = self.data_train.map(self.tf_encode)\n        self.data_train = self.data_train.filter(lambda x, y: tf.logical_and(tf.size(x) <= max_len, tf.size(y) <= max_len))\n        self.data_train = self.data_train.cache().shuffle(BUFFER_SIZE).padded_batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n        self.data_valid = self.data_valid.map(self.tf_encode)\n        self.data_valid = self.data_valid.filter(lambda x, y: tf.logical_and(tf.size(x) <= max_len, tf.size(y) <= max_len)).padded_batch(batch_size)\n\n    def tokenize_dataset(self, data):\n        tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((pt.numpy() for pt, en in data), target_vocab_size=2**15)\n        tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((en.numpy() for pt, en in data), target_vocab_size=2**15)\n        return tokenizer_pt, tokenizer_en\n\n    def encode(self, pt, en):\n        pt_tokens = [self.tokenizer_pt.vocab_size] + self.tokenizer_pt.encode(pt.numpy()) + [self.tokenizer_pt.vocab_size + 1]\n        en_tokens = [self.tokenizer_en.vocab_size] + self.tokenizer_en.encode(en.numpy()) + [self.tokenizer_en.vocab_size + 1]\n        return pt_tokens, en_tokens\n\n    def tf_encode(self, pt, en):\n        pt_tokens, en_tokens = tf.py_function(self.encode, [pt, en], [tf.int64, tf.int64])\n        pt_tokens.set_shape([None])\n        en_tokens.set_shape([None])\n        return pt_tokens, en_tokens",
      "outputs": []
    },
    {
      "id": "fa785b3f-7d35-4f72-9286-e799b4846639",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "28152f15-559f-4367-87dc-2c1eab99564c"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:24:06.362124+00:00",
          "start_time": "2023-08-22T06:24:06.198755+00:00"
        }
      },
      "execution_count": null,
      "source": "# Custom Learning Rate Schedule\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\n# Loss Function\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n\n# Train Step Function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)",
      "outputs": []
    },
    {
      "id": "7222d21c-0a20-4d02-aa60-7d60058e1c0b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "142f4f8b-ef2f-4bd9-9c4b-9462fd72de74"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:27:46.354067+00:00",
          "start_time": "2023-08-22T06:24:12.299281+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating Dataset instance with batch_size and max_len\nbatch_size = 64\nmax_len = 40\ndataset = Dataset(batch_size, max_len)\n\n# Model Parameters\nnum_layers = 4\nd_model = 128\nnum_heads = 8\ndff = 512\ninput_vocab_size = dataset.tokenizer_pt.vocab_size + 2\ntarget_vocab_size = dataset.tokenizer_en.vocab_size + 2\npe_input = 10000\npe_target = 6000\n\n# Transformer Model\ntransformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n\n# Learning Rate and Optimizer\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\n# Loss and Metrics\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')",
      "outputs": []
    },
    {
      "id": "c80c3fab-f364-4ac4-96f2-54c994d42472",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ff3ffe40-3dde-4e69-ba79-db29fdbf118d"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:28:24.512157+00:00",
          "start_time": "2023-08-22T06:28:24.313852+00:00"
        }
      },
      "execution_count": null,
      "source": "# Training Loop\nEPOCHS = 20\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    # inp -> Portuguese, tar -> English\n    for (batch, (inp, tar)) in enumerate(dataset.data_train):\n        train_step(inp, tar)\n\n        if batch % 50 == 0:\n            print(f'Epoch {epoch + 1}, Batch {batch}, Loss {train_loss.result()}, Accuracy {train_accuracy.result() * 100}')\n\n    print(f'Epoch {epoch + 1}, Loss {train_loss.result()}, Accuracy {train_accuracy.result() * 100}')\n    print(f'Time taken for 1 epoch: {time.time() - start} secs\\n')",
      "outputs": []
    },
    {
      "id": "7950b499-ebbf-4fa8-8b44-67c4a95819e5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d90f7af1-f06d-4bad-8f22-e4eef894f988"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:31:59.342670+00:00",
          "start_time": "2023-08-22T06:28:30.599472+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating Dataset instance with batch_size and max_len\nbatch_size = 64\nmax_len = 40\ndataset = Dataset(batch_size, max_len)\n\n# Model Parameters\nnum_layers = 4\nd_model = 128\nnum_heads = 8\ndff = 512\ninput_vocab_size = dataset.tokenizer_pt.vocab_size + 2\ntarget_vocab_size = dataset.tokenizer_en.vocab_size + 2\npe_input = 10000\npe_target = 6000\n\n# Transformer Model\ntransformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n\n# Learning Rate and Optimizer\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\n# Loss and Metrics\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')",
      "outputs": []
    },
    {
      "id": "a6458b5f-5e9c-48eb-8192-35602c00bd42",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "07bd78cb-84c5-43ea-ab3a-4e189224ebd7"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T04:18:01.204727+00:00",
          "start_time": "2023-08-22T04:18:01.013775+00:00"
        }
      },
      "execution_count": null,
      "source": "# Training Loop\nEPOCHS = 20\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    # inp -> Portuguese, tar -> English\n    for (batch, (inp, tar)) in enumerate(dataset.data_train):\n        train_step(inp, tar)\n\n        if batch % 50 == 0:\n            print(f'Epoch {epoch + 1}, Batch {batch}, Loss {train_loss.result()}, Accuracy {train_accuracy.result() * 100}')\n\n    print(f'Epoch {epoch + 1}, Loss {train_loss.result()}, Accuracy {train_accuracy.result() * 100}')\n    print(f'Time taken for 1 epoch: {time.time() - start} secs\\n')",
      "outputs": []
    },
    {
      "id": "a5c0bf06-f1c5-4d6f-bcb7-128370d9b5c3",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "c8133f74-957a-49eb-a66f-cc5cf62f85b1"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T04:18:48.792507+00:00",
          "start_time": "2023-08-22T04:18:48.631480+00:00"
        }
      },
      "execution_count": null,
      "source": "# Custom Learning Rate Schedule\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)",
      "outputs": []
    },
    {
      "id": "62b3b281-2338-47b9-a50f-bf02843388a0",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4e908e0c-3e2e-494a-8723-b268bc0e9ab9"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T04:19:07.389954+00:00",
          "start_time": "2023-08-22T04:19:07.232187+00:00"
        }
      },
      "execution_count": null,
      "source": "# Loss Function\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)",
      "outputs": []
    },
    {
      "id": "e5c30c74-b7e4-4987-b6a0-c39757f919af",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a84ac844-e293-42de-b488-470abb9502f4"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T04:19:32.849439+00:00",
          "start_time": "2023-08-22T04:19:32.689765+00:00"
        }
      },
      "execution_count": null,
      "source": "# Train Step Function\n@tf.function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)",
      "outputs": []
    },
    {
      "id": "6157e648-57c1-48ad-9edb-2f80e9f4de8d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "cdb489ac-7259-47d1-b49e-4da925a5e86d"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T04:20:11.929935+00:00",
          "start_time": "2023-08-22T04:20:11.730963+00:00"
        }
      },
      "execution_count": null,
      "source": "# Model Parameters\nnum_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\ninput_vocab_size = dataset.tokenizer_pt.get_vocab_size().numpy() + 2\ntarget_vocab_size = dataset.tokenizer_en.get_vocab_size().numpy() + 2\ndropout_rate = 0.1\n\n# Optimizer\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\n# Loss and Metrics\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\n# Transformer\ntransformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input=input_vocab_size, pe_target=target_vocab_size, rate=dropout_rate)",
      "outputs": []
    },
    {
      "id": "d5366924-075b-4f13-8b5a-00b53975c190",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "cf128224-338a-46f9-8fbf-140ed4456eae"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:18:49.912165+00:00",
          "start_time": "2023-08-22T06:18:49.745195+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install tensorflow tensorflow_datasets\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Dataset Class Definition\nclass Dataset:\n    def __init__(self, batch_size, max_len):\n        self.data_train = ted_hrlr_translate['train']\n        self.data_valid = ted_hrlr_translate['validation']\n        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n        self.data_train = self.data_train.map(self.tf_encode)\n        self.data_train = self.data_train.filter(lambda x, y: tf.logical_and(tf.size(x) <= max_len, tf.size(y) <= max_len))\n        self.data_train = self.data_train.cache()\n        self.data_train = self.data_train.shuffle(BUFFER_SIZE).padded_batch(batch_size)\n        self.data_train = self.data_train.prefetch(tf.data.experimental.AUTOTUNE)\n        self.data_valid = self.data_valid.map(self.tf_encode)\n        self.data_valid = self.data_valid.filter(lambda x, y: tf.logical_and(tf.size(x) <= max_len, tf.size(y) <= max_len)).padded_batch(batch_size)\n\n    def tokenize_dataset(self, data):\n        tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n            (pt.numpy() for pt, _ in data), target_vocab_size=2**15)\n        tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n            (en.numpy() for _, en in data), target_vocab_size=2**15)\n        return tokenizer_pt, tokenizer_en\n\n    def encode(self, pt, en):\n        pt_tokens = [self.tokenizer_pt.vocab_size] + self.tokenizer_pt.encode(pt.numpy()) + [self.tokenizer_pt.vocab_size + 1]\n        en_tokens = [self.tokenizer_en.vocab_size] + self.tokenizer_en.encode(en.numpy()) + [self.tokenizer_en.vocab_size + 1]\n        return pt_tokens, en_tokens\n\n    def tf_encode(self, pt, en):\n        pt_tokens, en_tokens = tf.py_function(self.encode, [pt, en], [tf.int64, tf.int64])\n        pt_tokens.set_shape([None])\n        en_tokens.set_shape([None])\n        return pt_tokens, en_tokens",
      "outputs": []
    },
    {
      "id": "e70ba022-a4a3-48f0-8e49-a3ee9fc5bfa9",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "7674d213-94d4-4094-9c1a-6233c543991e"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T06:18:50.148516+00:00",
          "start_time": "2023-08-22T06:18:49.920198+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating an instance of the Dataset class\nBUFFER_SIZE = 20000\nBATCH_SIZE = 64\nMAX_LENGTH = 40\n\ndataset = Dataset(BATCH_SIZE, MAX_LENGTH)",
      "outputs": []
    },
    {
      "id": "4b5980b8-3cf0-4c22-9359-fedbcc15b8f2",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4a5917d8-4cd1-4e96-b54e-a600db593e90"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T04:40:41.061761+00:00",
          "start_time": "2023-08-22T04:40:40.758667+00:00"
        }
      },
      "execution_count": null,
      "source": "# Loading the TED Talks translation dataset (Portuguese to English)\nted_hrlr_translate = tfds.load(name='ted_hrlr_translate/pt_to_en', as_supervised=True)",
      "outputs": []
    },
    {
      "id": "adf66179-d67c-45f3-8d84-fa835d384610",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "95e7b019-8999-4cfa-8b15-34a5cd8577bf"
        },
        "ExecuteTime": {
          "end_time": "2023-08-22T04:58:11.958800+00:00",
          "start_time": "2023-08-22T04:54:33.102115+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating an instance of the Dataset class with the specified batch size and maximum length\nBUFFER_SIZE = 20000\nBATCH_SIZE = 64\nMAX_LENGTH = 40\n\ndataset = Dataset(BATCH_SIZE, MAX_LENGTH)",
      "outputs": []
    },
    {
      "id": "aacee9bc-2f4a-4c34-9fc4-b19cb90f32ab",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "fd3105d8-8da8-4a7c-b8e4-2abaa5f997da"
        }
      },
      "execution_count": null,
      "source": "# Defining the model parameters\nN = 2  # Number of blocks in the encoder and decoder\ndm = 256  # Dimensionality of the model\nh = 8  # Number of heads\nhidden = 512  # Number of hidden units in the fully connected layers\nmax_len = 40  # Maximum number of tokens per sequence\nbatch_size = 64  # Batch size for training\nepochs = 3  # Number of epochs to train for",
      "outputs": []
    }
  ]
}