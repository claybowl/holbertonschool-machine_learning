# 0x01. Classification

By: Alexa Orrico, Software Engineer at Holberton School

## Project Information
- Weight: 3
- Project start date: Feb 3, 2023 12:00 AM
- Project end date: Feb 9, 2023 12:00 AM
- Release date: Feb 3, 2023 12:00 AM
- Manual QA review: Request when you are done with the project
- Auto review: Will be launched at the deadline

## Background Context
At the end of this project, you should be able to build your own binary image classifier from scratch using numpy.

## Good luck and have fun!

## Resources

### Read or watch:
- Supervised vs. Unsupervised Machine Learning
- How would you explain neural networks to someone who knows very little about AI or neurology?
- Using Neural Nets to Recognize Handwritten Digits (until “A simple network to classify handwritten digits” (excluded))
- Forward propagation
- Understanding Activation Functions in Neural Networks
- Loss function
- Gradient descent
- Calculus on Computational Graphs: Backpropagation
- Backpropagation calculus
- What is a Neural Network?
- Supervised Learning with a Neural Network
- Binary Classification
- Logistic Regression
- Logistic Regression Cost Function
- Gradient Descent
- Computation Graph
- Logistic Regression Gradient Descent
- Vectorization
- Vectorizing Logistic Regression
- Vectorizing Logistic Regression’s Gradient Computation
- A Note on Python/Numpy Vectors
- Neural Network Representations
- Computing Neural Network Output
- Vectorizing Across Multiple Examples
- Gradient Descent For Neural Networks
- Random Initialization
- Deep L-Layer Neural Network
- Train/Dev/Test Sets
- Random Initialization For Neural Networks : A Thing Of The Past
- Initialization of deep networks
- Multiclass classification
- Derivation: Derivatives for Common Neural Network Activation Functions
- What is One Hot Encoding? Why And When do you have to use it?
- Softmax function
- What is the intuition behind SoftMax function?
- Cross entropy
- Loss Functions: Cross-Entropy
- Softmax Regression (Note: I suggest watching this video at 1.5x - 2x speed)
- Training Softmax Classifier (Note: I suggest watching this video at 1.5x - 2x speed)
- numpy.zeros
- numpy.random.randn
- numpy.exp
- numpy.log
- numpy.sqrt
- numpy.where
- numpy.max
- numpy.sum
- numpy.argmax
- What is Pickle in python?
- pickle
- pickle.dump
- pickle.load

### Optional:
- Predictive analytics
- Maximum Likelihood Estimation

![image](https://user-images.githubusercontent.com/103296303/216742528-94c04b7a-6e81-48b8-9540-84a3d240eb82.png)

## Learning Objectives
At the end of this project, you are expected to be able to explain to anyone, without the help of Google:
- What is a model?
- What is supervised learning?
- What is a prediction?
- What is a node?
- What is a weight?
- What is a bias?
- What are activation functions?
- Sigmoid?
- Tanh?
- Relu?
- Softmax?
- What is a layer?
- What is a hidden layer?
- What is Logistic Reg
